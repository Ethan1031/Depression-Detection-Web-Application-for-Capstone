{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "3HGFHmGI1MHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mne\n",
        "\n",
        "def reject_criteria(x):\n",
        "\n",
        "  max_condition = np.max(x, axis = 1) > 1e-4 # Condition for if the maximum value of the array is above 0.0001 V\n",
        "  min_condition = np.min(x, axis = 1) < -1e-4 # Condition for if the minimum value of the array is below -0.0001 V\n",
        "\n",
        "  # Returns the result for the given array. Whether to reject or not the array if a condition is true.\n",
        "  return ((max_condition.any() or min_condition.any()), [\"max amp\", \"min amp\"])\n",
        "\n",
        "# This function takes in the file path of an .edf file and preprocesses the file accordingly to our needs\n",
        "def read_data(file_path):\n",
        "\n",
        "  # To turn the edf file into a raw object so that mne functions can be applied to the data\n",
        "  data = mne.io.read_raw_edf(file_path, preload = True)\n",
        "\n",
        "  # This setting sets the raw object to understand that the data is on eeg\n",
        "  data.set_eeg_reference()\n",
        "\n",
        "  # Bandpass filters are applied using this function on the data. Low pass filter of 0.1 Hz and High pass filter of 70 Hz is used\n",
        "  data.filter(l_freq = 0.1, h_freq = 70)\n",
        "\n",
        "  # Notch filter of 50 Hz is applied on the data using the function below\n",
        "  data.notch_filter(50)\n",
        "\n",
        "  # The channels used for each patient can vary from 20 to 22 channels.\n",
        "  # The conditions below are used to removed targeted channels as doing so will improve the performance of the deep learning model\n",
        "  # Channels 'EEG 23A-23R' and 'EEG 24A-24R' are reference channels, while the other channels are selected as removing them improved performance\n",
        "  if data.info['nchan'] == 22:\n",
        "    data.drop_channels(['EEG 23A-23R', 'EEG 24A-24R', 'EEG A2-A1', 'EEG C3-LE', 'EEG F4-LE'])\n",
        "  if data.info['nchan'] == 20:\n",
        "    data.drop_channels(['EEG A2-A1', 'EEG C3-LE', 'EEG F4-LE'])\n",
        "\n",
        "  data.rename_channels({\n",
        "    'EEG Fp1-LE': 'Fp1', 'EEG Fp2-LE': 'Fp2',\n",
        "    'EEG F7-LE': 'F7', 'EEG F3-LE': 'F3', 'EEG Fz-LE': 'Fz', 'EEG F8-LE': 'F8',\n",
        "    'EEG T3-LE': 'T3', 'EEG Cz-LE': 'Cz', 'EEG C4-LE': 'C4', 'EEG T4-LE': 'T4',\n",
        "    'EEG T5-LE': 'T5', 'EEG P3-LE': 'P3', 'EEG Pz-LE': 'Pz', 'EEG P4-LE': 'P4', 'EEG T6-LE': 'T6',\n",
        "    'EEG O1-LE': 'O1', 'EEG O2-LE': 'O2'\n",
        "    })\n",
        "  data.set_montage(mne.channels.make_standard_montage(\"standard_1020\"))\n",
        "\n",
        "  # Independent Component Analysis (ICA) is applied here to help reduce the noise in the signals\n",
        "  ica = mne.preprocessing.ICA(random_state = 42, n_components = 13)\n",
        "\n",
        "  # The ICA is fitted to a copy of the data that has is filtered using a low pass filter of 1 Hz as ICA does not work well with low frequencies\n",
        "  ica.fit(data.copy().filter(l_freq = 1.0, h_freq = None))\n",
        "\n",
        "  data = ica.apply(data.copy()) # The fitted ICA is then applied on the data\n",
        "\n",
        "  # Segmentation of signals is done by creating fixed length epochs that are 10 seconds each.\n",
        "  epochs = mne.make_fixed_length_epochs(data, duration = 5, overlap = 2)\n",
        "\n",
        "  # Using the reject criteria function made above, we can detect segments of the signals that are noisy and drop them from the data we want to use, cleaning the dataset\n",
        "  epochs.drop_bad(reject = dict(eeg=reject_criteria))\n",
        "\n",
        "  # This function returns the data from epoches in the form of an np array\n",
        "  array = epochs.get_data()\n",
        "\n",
        "  # Returns the preprocessed data\n",
        "  return array\n",
        "\n",
        "# Using list comprehension, each of the file paths are given in the read data function and their respective preprocessed data is extracted and stored in the proper list\n",
        "h_epochs_list = [read_data(i) for i in h_file_paths]\n",
        "mdd_epochs_list = [read_data(i) for i in mdd_file_paths]\n",
        "\n",
        "h_epochs_labels = [len(i) * [0] for i in h_epochs_list]\n",
        "mdd_epochs_labels = [len(i) * [1] for i in mdd_epochs_list]\n",
        "\n",
        "data_list = h_epochs_list + mdd_epochs_list\n",
        "label_list = h_epochs_labels + mdd_epochs_labels\n",
        "\n",
        "data_array = np.vstack(data_list)\n",
        "\n",
        "label_array = np.hstack(label_list)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# We need to reshape the 3D array into a 1D array too allow the scaler to fit to all of the data points\n",
        "original_shape = data_array.shape # Saving the original shape of the array\n",
        "\n",
        "data_array = data_array.reshape(-1, 1) # Reshaping the array into a 1D array\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data_array) # Fitting the scaler and saving the scaled data array in a new variable\n",
        "\n",
        "data_array = scaled_data.reshape(original_shape)"
      ],
      "metadata": {
        "id": "Y5gyC01y1L6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction"
      ],
      "metadata": {
        "id": "CRWmaDv91oP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_signals = [] # list to hold the means of the data\n",
        "\n",
        "for d in data_array:\n",
        "  mean_signals.append(np.mean(d, axis = 0))\n",
        "\n",
        "from scipy import signal\n",
        "\n",
        "X_data = [] # This will be the list that will hold the image data\n",
        "sequence_length = 0\n",
        "image_shape = []\n",
        "\n",
        "import cv2\n",
        "\n",
        "# The loop goes through each of the segment and adds the spectrogram data of the segments to the list\n",
        "for d in mean_signals:\n",
        "\n",
        "  # To create the spectrogram, the STFT data is first generated from our data\n",
        "  frequencies, times, stft_data = signal.spectrogram(x = d, fs = 256, window=('tukey', 0.25), nperseg=32, noverlap=16, nfft=32)\n",
        "\n",
        "  # The generated stft data is then resized accordingly such that it holds proper image data as a spectrogram\n",
        "  resized_spectrogram = np.abs(stft_data)\n",
        "  resized_spectrogram = resized_spectrogram / np.max(resized_spectrogram)\n",
        "\n",
        "  resized_spectrogram = cv2.resize(resized_spectrogram, dsize=(75, 17), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "  split_array = np.split(resized_spectrogram, 3, axis=1)\n",
        "  spectrogram_array = np.array(split_array)\n",
        "\n",
        "  X_data.append(spectrogram_array)\n",
        "\n",
        "X_data = np.array(X_data)"
      ],
      "metadata": {
        "id": "aougEFz21p3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "DgbuS_ZT2EiZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi8DzMf0AGPW"
      },
      "outputs": [],
      "source": [
        "# As the data of each patient is stored in their respective file, we must first be able to access them individually\n",
        "# To do so we need to get their relative file paths and store them in a list for easy access\n",
        "\n",
        "# The os library is imported as it's functions are needed to extract and handle file paths\n",
        "import os\n",
        "\n",
        "folder_path = \"/mumtaz_dataset\" # This is the directory path where files of the dataset is located\n",
        "\n",
        "# Using list comprehensions, a list is made where each element is a file path to a file in the dataset\n",
        "\n",
        "# The code below creates a list with the name of all the files from the dataset IF the file is checked to be a file\n",
        "file_paths = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "\n",
        "# The code below then joins the name of the files from the dataset with the path of the directory.\n",
        "# This creates a list of the all relative file path for the dataset files\n",
        "file_paths = [os.path.join(folder_path, f) for f in file_paths]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Healthy and Major Depressive Disorder files should be seperated since they will be labeled differently for our deep learning model to recognize\n",
        "# These files are seperated to allow labeling to be much simpler\n",
        "\n",
        "# Seperate lists for the healthy (h) and major depressive disorder (mdd) data\n",
        "h_file_paths = []\n",
        "mdd_file_paths = []\n",
        "\n",
        "# This loop goes through the list containing all of the file paths of the dataset files and by checking the file names,\n",
        "# places the relative file paths accordingly to the respective list.\n",
        "# Eyes Closed (EC) files are used as the Stimuli chosen to teach the model with\n",
        "for i in file_paths:\n",
        "\n",
        "  # Gets the file name from the relative file path as it splits the path where there is a '/' and\n",
        "  file_name = i.split('/')[-1]\n",
        "\n",
        "  # If the file name has 'EC' in it, it means that the patient had their eyes closed and it continues\n",
        "  if ('EC' in file_name):\n",
        "\n",
        "    # If there is a 'H' in the file name, it means that it is a file for a Healthy patient and adds that file's relative path to the healthy list\n",
        "    if ('H' in file_name):\n",
        "      h_file_paths.append(i)\n",
        "\n",
        "    # If there is a 'M' in the file name, it means that it is a file for a Major Depressive Disorder patient and adds that file's relative path to the major depressive disorder list\n",
        "    if ('M' in file_name):\n",
        "      mdd_file_paths.append(i)"
      ],
      "metadata": {
        "id": "Rwets_d-AVTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All of the files are in the .edf file format. .edf files are able to hold a lot of useful information for our needs,\n",
        "# thus to effectively make use of that data, the mne library is imported\n",
        "import mne\n",
        "\n",
        "# This function is used in the next function. It is used to help clean what is considered noisy signal from the data\n",
        "# Amplitudes for EEG signals ranges from 0 - 100 microVolts (ÂµV) (https://www.researchgate.net/publication/339736502_Classification_of_Brainwaves_for_Sleep_Stages_by_High-Dimensional_FFT_Features_from_EEG_Signals)\n",
        "# Since the data type of the signals is in V, we will consider any values above 0.0001 V (1e-4) or below -0.0001 V (-1e-4) to be noise\n",
        "def reject_criteria(x):\n",
        "\n",
        "  max_condition = np.max(x, axis = 1) > 1e-4 # Condition for if the maximum value of the array is above 0.0001 V\n",
        "  min_condition = np.min(x, axis = 1) < -1e-4 # Condition for if the minimum value of the array is below -0.0001 V\n",
        "\n",
        "  # Returns the result for the given array. Whether to reject or not the array if a condition is true.\n",
        "  return ((max_condition.any() or min_condition.any()), [\"max amp\", \"min amp\"])\n",
        "\n",
        "# This function takes in the file path of an .edf file and preprocesses the file accordingly to our needs\n",
        "def read_data(file_path):\n",
        "\n",
        "  # To turn the edf file into a raw object so that mne functions can be applied to the data\n",
        "  data = mne.io.read_raw_edf(file_path, preload = True)\n",
        "\n",
        "  # This setting sets the raw object to understand that the data is on eeg\n",
        "  data.set_eeg_reference()\n",
        "\n",
        "  # Bandpass filters are applied using this function on the data. Low pass filter of 0.1 Hz and High pass filter of 70 Hz is used\n",
        "  data.filter(l_freq = 0.1, h_freq = 70)\n",
        "\n",
        "  # Notch filter of 50 Hz is applied on the data using the function below\n",
        "  data.notch_filter(50)\n",
        "\n",
        "  # The channels used for each patient can vary from 20 to 22 channels.\n",
        "  # The conditions below are used to removed targeted channels as doing so will improve the performance of the deep learning model\n",
        "  # Channels 'EEG 23A-23R' and 'EEG 24A-24R' are reference channels, while the other channels are selected as removing them improved performance\n",
        "  if data.info['nchan'] == 22:\n",
        "    data.drop_channels(['EEG 23A-23R', 'EEG 24A-24R', 'EEG Fz-LE', 'EEG Cz-LE', 'EEG Pz-LE'])\n",
        "  if data.info['nchan'] == 20:\n",
        "    data.drop_channels(['EEG Fz-LE', 'EEG Cz-LE', 'EEG Pz-LE'])\n",
        "\n",
        "  # Independent Component Analysis (ICA) is applied here to help reduce the noise in the signals\n",
        "  ica = mne.preprocessing.ICA(random_state = 42, n_components = 13)\n",
        "\n",
        "  # The ICA is fitted to a copy of the data that has is filtered using a low pass filter of 1 Hz as ICA does not work well with low frequencies\n",
        "  ica.fit(data.copy().filter(l_freq = 1.0, h_freq = None))\n",
        "\n",
        "  data = ica.apply(data.copy()) # The fitted ICA is then applied on the data\n",
        "\n",
        "  # Segmentation of signals is done by creating fixed length epochs that are 10 seconds each.\n",
        "  epochs = mne.make_fixed_length_epochs(data, duration = 10, overlap = 2)\n",
        "\n",
        "  # Using the reject criteria function made above, we can detect segments of the signals that are noisy and drop them from the data we want to use, cleaning the dataset\n",
        "  epochs.drop_bad(reject = dict(eeg=reject_criteria))\n",
        "\n",
        "  # This function returns the data from epoches in the form of an np array\n",
        "  array = epochs.get_data()\n",
        "\n",
        "  # Returns the preprocessed data\n",
        "  return array"
      ],
      "metadata": {
        "id": "rWQ-HdJMAdLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using list comprehension, each of the file paths are given in the read data function and their respective preprocessed data is extracted and stored in the proper list\n",
        "h_epochs_list = [read_data(i) for i in h_file_paths]\n",
        "mdd_epochs_list = [read_data(i) for i in mdd_file_paths]"
      ],
      "metadata": {
        "id": "-_us5wLqAf0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the preprocessed data lists, their respective labels are made. 0 meaning Healthy patients and 1 meaning Major Depressive Disorder patients\n",
        "# These values are stored in their appropriate lists\n",
        "h_epochs_labels = [len(i) * [0] for i in h_epochs_list]\n",
        "mdd_epochs_labels = [len(i) * [1] for i in mdd_epochs_list]"
      ],
      "metadata": {
        "id": "XxwBKkfLAgz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we have the data and labels for both Healthy and Major Depressive Disorder data, we can merge the data together to a collective list\n",
        "# Now we have a list containing all of our data and another list containing all of our labels.\n",
        "# The same indexes of the lists are related, for example index 1 of the data list has its corespondiing label of index 1 in the label list\n",
        "# The reason this combining step is done is so that the data can be randomly split properly\n",
        "data_list = h_epochs_list + mdd_epochs_list\n",
        "label_list = h_epochs_labels + mdd_epochs_labels"
      ],
      "metadata": {
        "id": "y-cuQcWmAiyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# At the moment the data and labels are stored within lists. We need to convert them into np arrays such that it can be manipulated more efficiently using numpy functions\n",
        "# The numpy library is imported to make use of its function\n",
        "import numpy as np\n",
        "\n",
        "# Stacking the data list vertically, allowing a 3 dimensional array to be made of the data.\n",
        "# First dimension is the number of segments of the data\n",
        "# Second dimension is the number of channels kept for each segment\n",
        "# Third dimension is time. As the sampling rate is 256 Hz, over 10 seconds there are 2560 data points for each channel\n",
        "data_array = np.vstack(data_list)\n",
        "\n",
        "# Stacking the data horizontally, creating a 1 dimensional array containing all of the labels for each of the segments\n",
        "label_array = np.hstack(label_list)"
      ],
      "metadata": {
        "id": "nl7NrTlqAktn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another preprocessing step to take is to scale the data using z-score normalization\n",
        "# Doing so allows all of the preprocessed data to have the same scale and prevent certain data points from dominating over others\n",
        "# The StandardScaler function from sklearn is used to apply this step\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# We need to reshape the 3D array into a 1D array too allow the scaler to fit to all of the data points\n",
        "original_shape = data_array.shape # Saving the original shape of the array\n",
        "\n",
        "data_array = data_array.reshape(-1, 1) # Reshaping the array into a 1D array\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data_array) # Fitting the scaler and saving the scaled data array in a new variable\n",
        "\n",
        "#data_array = scaled_data.reshape(1853, 17, 2560)\n",
        "# Reshaping the scaled data array back to the original shape\n",
        "data_array = scaled_data.reshape(original_shape)"
      ],
      "metadata": {
        "id": "RGvI0OZEAmab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Having 17 channels to extract features from can lead to difficulties in creating the proper input to our deep learning model\n",
        "# Therefore, the mean of the 17 channels will be used instead\n",
        "\n",
        "mean_signals = [] # list to hold the means of the data\n",
        "\n",
        "# The loop below loops over the data array and gets the mean value between all 17 channels for that point of time\n",
        "# This results in a 2 dimensional array,\n",
        "# with the first dimension being the segments of the data and the second dimension being the time dimension, with the values of each point being the mean of the 17 channels\n",
        "for d in data_array:\n",
        "  mean_signals.append(np.mean(d, axis = 0))"
      ],
      "metadata": {
        "id": "udacWpZKAsNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The feature that will be extracted from the preprocessed data is an image, specifically a spectrogram\n",
        "# This can be achieved using the functions in the scipy library\n",
        "from scipy import signal\n",
        "\n",
        "n_freq_bins = 256\n",
        "n_time_bins = 384\n",
        "\n",
        "X_data = [] # This will be the list that will hold the image data\n",
        "\n",
        "# The loop goes through each of the segment and adds the spectrogram data of the segments to the list\n",
        "for d in mean_signals:\n",
        "\n",
        "  # To create the spectrogram, the STFT data is first generated from our data\n",
        "  frequencies, times, stft_data = signal.spectrogram(x = d, fs = 256, window = signal.windows.tukey(64, 0.25))\n",
        "\n",
        "  # The generated stft data is then resized accordingly such that it holds proper image data as a spectrogram\n",
        "  resized_spectrogram = np.abs(stft_data)\n",
        "  resized_spectrogram = resized_spectrogram[:n_freq_bins, :n_time_bins]\n",
        "  resized_spectrogram = resized_spectrogram / np.max(resized_spectrogram)\n",
        "\n",
        "  X_data.append(resized_spectrogram) # Adds to the data list\n",
        "\n",
        "X_data = np.array(X_data) # Converts the list into a np array"
      ],
      "metadata": {
        "id": "szJLmLxlAuKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As the features are now prepared to develop the model, the data needs to be split into a training and test datasets\n",
        "# This will be done using the train test split function\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, label_array, stratify = label_array, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "IgPvYW6jAweD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the deep learning model tensorflow will be used as the library to create it\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# This function below creates the deep learning model based its architecture\n",
        "def create_model():\n",
        "\n",
        "  # As images are being used as the input to the model, 2D convolutional layers are utilized\n",
        "  # This architecture is obtained through trial and error, whilst using grid search to tune it's hyperparameters\n",
        "  model = keras.Sequential([\n",
        "    keras.layers.Conv2D(128, kernel_size = (3, 3), activation = 'relu', input_shape = (33, 45, 1)),\n",
        "    keras.layers.MaxPooling2D(pool_size = (2, 2)),\n",
        "    keras.layers.Conv2D(64, kernel_size = (3, 3), activation = 'relu'),\n",
        "    keras.layers.MaxPooling2D(pool_size = (2, 2)),\n",
        "    keras.layers.Conv2D(32, kernel_size = (2, 2), activation = 'relu'),\n",
        "    keras.layers.MaxPooling2D(pool_size = (2, 2)),\n",
        "    keras.layers.Reshape((2, 4 * 32)), # Reshaping the data from Pooling to be suitable for LSTM\n",
        "    keras.layers.LSTM(128), # LSTM is used to better understand temporal patterns found in the data\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(64, activation = 'relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(32, activation = 'relu'),\n",
        "    keras.layers.Dense(2, activation = 'sigmoid') # A final dense layer with an output of 2 is used as our model has a binary output (Healthy and Major Depressive Disorder)\n",
        "  ])\n",
        "\n",
        "  # Adam is used as the optimizer and sparse categorical crossentropy is used as the loss function since they produced the best results\n",
        "  model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "GjYLdJ_aAygg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model can now be trained using the whole dataset accordingly\n",
        "model = create_model() # Creating the model\n",
        "\n",
        "# Training the model with training data\n",
        "model.fit(X_train, y_train, epochs = 12, batch_size = 4, validation_split = 0.2)"
      ],
      "metadata": {
        "id": "JX_0-CxjA1m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To check of it's performance using classfication metrics, the classfication report function is utilized\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predictions = np.argmax(model.predict(X_test), axis = 1) # Predictions of the model on the testing data is made\n",
        "# The report is shown which consists of Precision, Recall, F1-Score and Accuracy\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "id": "uzdDqDv2CHwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model in the h5 file format so that it can be deployed\n",
        "model.save('model.h5')"
      ],
      "metadata": {
        "id": "woLIusLTB-HF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}